# -*- coding: utf-8 -*-
"""2_AnomalyDetection_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sa6D6UscARP5ygxcmwtpcFIX3ETM1c1B
"""

from google.colab import files
uploaded = files.upload()

""">  Main Libraries"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

"""# 1- Main Functions (Pipeline)

## a) Printable Functions

### (i) DF Shape
"""

def shape(df):
  # DataFrame shape
  print('-='*30)
  print('|\t\t\t| DF Shape |\t\t\t')
  print('|')
  print(f'|  Rows: {df.shape[0]}\n|   X\n|  Columns: {df.shape[1]}')
  print('-='*30)
  return None

"""### (ii) DF Info"""

def info(df):
  print(df.info(verbose=True))
  return None

"""### (iii) Nulls Quantity"""

def nulls_qtd(df):
  print(df.isna().sum())
  return None

"""### (iv) Data Description"""

def describe(df, perc, drop_list = None):
  if drop_list == None:
    df.select_dtypes(include='number').describe(percentiles = perc)
  else:
    df.select_dtypes(include='number').drop(drop_list, axis=1).describe(percentiles = perc)
  return None

"""## b) Transformable Functions

### (i) Changing Columns Dtype
"""

# Changes the variable type
def change_type(df, feature, ast = object):
  """
  df: Pandas DataFrame to be modified
  feature: a list or tuple of columns to be modified
  ast: set column astype (if not declared, automatically it will be setted as an object); must be a list or a tuple
  """
  try:
    len(feature) > 1
    len(ast) > 1
  except:
    print('function developed for multiple features changes.. You must input a list of each one')
    

  assert len(feature) == len(ast)

  for f, a in zip(feature, ast):
    f = str(f)
    a = str(a)
    df[f] = df[f].astype(a)
  return df

"""### (ii) Selecting DF Columns"""

def select_columns(df, columns_names_list):
  df_new = df[columns_names_list]
  return df_new

"""### (iii) Splitting data into train and test"""

def split_train_test(X, y, perc = 0.2):
  """
  X: x's variables
  y: target variable (system response)
  perc: test data percentage (decimal value; default = 0.2 = 20% of data)
  """
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

  # Resets all DFs indexes
  X_train.reset_index(drop = True, inplace = True)
  X_test.reset_index(drop = True, inplace = True)
  y_train.reset_index(drop = True, inplace = True)
  y_test.reset_index(drop = True, inplace = True)

  return X_train, X_test, y_train, y_test

"""## c) Mathematical and Statistics Functions

### (i) Data Scaling
"""

# Criar a função para o escalonamento dos dados
def feat_scale(X):

  # Calcular a média de todas as variáveis
  mu = np.mean(X, axis = 0)

  # Calcular o devio padrão de todas as variáveis
  sigma = np.std(X, axis=0, ddof=1) # ddof=1 calcula o desvio padrão amostral

  # Fazer os escalonamento das variáveis
  xNorm = (X - mu)/sigma

  return xNorm

"""### (ii) OLS Equation"""

def ols_formula(df, dependent_var, *excluded_rows):

  # Listar o nome das colunas do dataframe
  dfCols = list(df.columns.values)

  # Remover a variável dependente
  dfCols.remove(dependent_var)

  # Remover as variáveis excluídas
  for col in excluded_rows:
    dfCols.remove(col)

  # Retornar a fórmula
  return dependent_var + ' ~ ' + ' + '.join(dfCols)

"""### (iii) Gradient-Descent"""

def funcao_perda(X, y, beta):

  """
  X é a matriz com os regressores (m x n).
  y é a série com a resposta (1 x m).
  beta é a série dos parâmetros (1 x n).
  """

  # Previsão da resposta
  pred = X.dot(beta) #-> produto escalar

  # Calcular os erros de previsão
  res = np.subtract(pred, y.squeeze())

  # Calcular o quadrado dos erros
  sqrRes = np.square(res)

  # Calcular a perda
  perda = 1/(2*m) * np.sum(sqrRes)

  return perda

def grad_des(X, y, beta, alpha, itera):

  """
  X é a matriz dos regressores (m x n).
  y é a série da resposta (1 x m).
  beta é a série dos valores iniciais dos parâmetros (1 x n).
  alpha é a taxa de aprendizagem ou learning rate (escalar).
  itera é o número de iterações do algoritmo.

  __________________________________________
  Resultados

  beta é a série com os valores finais dos parâmetros.
  hperda é a série histórica das perdas.

  """

  # Inicializa a série do histórico de perdas
  hPerda = np.zeros(itera)

  for i in range(itera):

    # Calcula as previsões com os valores atuais dos parâmetros
    pred = X.dot(beta)

    # Calcula os resíduos da previsão
    res = np.subtract(pred, y.squeeze())

    # Calcula o incremento / decremento no valor dos betas - derivada parcial da função perda
    sumDelta = (alpha / m) * X.transpose().dot(res)

    # Atualizar os valores dos betas do modelo
    beta = beta - sumDelta

    #Calcula a nova perda com os novos valores dos betas
    hPerda[i] = funcao_perda(X, y, beta)

  return beta, hPerda, sumDelta

"""# 2- Exploratory Data Analysis (EDA)"""

df = pd.read_csv('RawData.csv',sep=';')
df.head()

df.pipe(shape)

df.pipe(info)

df.pipe(nulls_qtd)

plt.figure(figsize=(20,12))
sns.heatmap(df.drop(['airSpeed'], axis=1).corr(), annot=True, cmap='jet')

contColNames = list(df.drop(['airSpeed'], axis=1).select_dtypes(include='number').columns) #-> seleciona apenas colunas numéricas
ncols = 3 #-> número de colunas que armazenarão os plots na figure 
nrows = int(np.ceil(len(contColNames)/(1.0 * ncols))) #-> número de linhas que armazenarão os plots na figure 


fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(27,16))

counter = 0 

for i in range(nrows):
  for j in range(ncols):

    ax = axes[i][j]

    # Plotar somente quando houver dados
    if counter < len(contColNames):

      ax.hist(df.drop(['airSpeed'], axis=1).select_dtypes(include='number')[contColNames[counter]], bins=20)
      ax.set_xlabel(contColNames[counter])
      ax.set_ylabel('Frequência')

    else:
      ax.set_axis_off()

    counter += 1

plt.show()

contColNames = list(df.drop(['airSpeed'], axis=1).select_dtypes(include='number').columns) #-> seleciona apenas colunas numéricas
ncols = 3 #-> número de colunas que armazenarão os plots na figure 
nrows = int(np.ceil(len(contColNames)/(1.0 * ncols))) #-> número de linhas que armazenarão os plots na figure 


fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(27,16))

counter = 0 

for i in range(nrows):
  for j in range(ncols):

    ax = axes[i][j]

    # Plotar somente quando houver dados
    if counter < len(contColNames):
      red_square = dict(markerfacecolor='r', marker='s')
      ax.boxplot(df.drop(['airSpeed'], axis=1).select_dtypes(include='number')[contColNames[counter]], flierprops=red_square, vert=False, whis=0.75)
      ax.set_xlabel(contColNames[counter])
      ax.set_ylabel('Frequência')

    else:
      ax.set_axis_off()

    counter += 1

plt.show()

sns.distplot(df.pitch)

sns.distplot(df.pitchRate)

import plotly.express as px
fig = px.scatter_3d(df, x='rollRate', y='pitchRate', z='yawRate', color='climbRate', color_continuous_scale='inferno')
fig.show()

"""# 3- Anomaly Detection (LSTM)

> Specific Libraries
"""

#Importing Required Libraries
import pandas as pd
import numpy as np
import matplotlib
import seaborn
import matplotlib.dates as md
from matplotlib import pyplot as plt
import scipy as sp
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from keras.layers.core import Dense, Activation, Dropout
from keras.layers.recurrent import LSTM
from keras.models import Sequential
from keras.layers import Bidirectional
from keras.layers import Conv1D
import time

"""> Selecting Features"""

df_reduced = df.pipe(select_columns, ['rollRate', 'pitchRate', 'yawRate'])
df_reduced

"""> Feature Scaling"""

#Preparing the data for LSTM model
data_n = df_reduced
min_max_scaler = preprocessing.StandardScaler()
np_scaled = min_max_scaler.fit_transform(data_n)
data_n = pd.DataFrame(np_scaled)
data_n

"""> Principal Component Analysis (PCA)"""

df_pca_input = df.pipe(select_columns, ['roll','pitch','heading','rollRate','pitchRate','yawRate','climbRate','groundSpeed'])
df_pca_input

min_max_scaler = preprocessing.StandardScaler()
np_scaled = min_max_scaler.fit_transform(df_pca_input)
data_scaled_pca = pd.DataFrame(np_scaled)
data_scaled_pca

from sklearn.decomposition import PCA

pca = PCA(n_components = 3)
pca.fit(data_scaled_pca)

df_pca = pca.transform(data_scaled_pca)
df_pca

"""> LSTM Model"""

#Important parameters and training/Test size
prediction_time = 1 
# testdatasize = 540 
# testdatasize = 4325 
testdatasize = 8800
unroll_length = 180
testdatacut = testdatasize + unroll_length  + 1

#Training data
# X_train = data_n[0 : -9024].values   #selects all 5 columns
X_train = data_scaled_pca[0 : (-prediction_time - testdatacut)].values   #selects all 5 columns

# y_train = data_n[1 : -9023][0].values #targeted the "value" column as main feature
y_train = data_scaled_pca[prediction_time : -testdatacut][0].values #targeted the "value" column as main feature

#Test data
# X_test = data_n[-9023 : -1].values
X_test = data_scaled_pca[ (0-testdatacut) : -prediction_time].values

# y_test = data_n[-9022 : ][0].values
y_test = data_scaled_pca[(prediction_time - testdatacut) : ][0].values


#Shape of the data
print("x_train", X_train.shape)
print("y_train", y_train.shape)
print("x_test", X_test.shape)
print("y_test", y_test.shape)

sz = X_train.shape[0] + X_test.shape[0]

print(f'Train relation: {(X_train.shape[0]/sz)*100}% of the data')
print(f'Test relation: {(X_test.shape[0]/sz)*100}% of the data')

def unroll(data,sequence_length=180):  #sequence_length=24
    result = []
    for index in range(len(data) - sequence_length):
        result.append(data[index: index + sequence_length])
    return np.asarray(result)

#Adapt the datasets for the sequence data shape
X_train = unroll(X_train,unroll_length)
X_test  = unroll(X_test,unroll_length)
y_train = y_train[-X_train.shape[0]:]
y_test  = y_test[-X_test.shape[0]:]


#Shape of the data
print("X_train", X_train.shape)
print("y_train", y_train.shape)
print("X_test", X_test.shape)
print("y_test", y_test.shape)

X_test

y_test

"""> Train"""

#Building the model
model = Sequential()

# model.add(LSTM(units=180, input_dim=x_train.shape[-1], return_sequences=True))
# model.add(Dropout(0.2))

model.add(Bidirectional(LSTM(units=180, input_dim=X_train.shape[-1], return_sequences=False)))
model.add(Dropout(0.2))

# model.add(LSTM(360, return_sequences=False))
# model.add(Dropout(0.2))

model.add(Dense(units=1))
model.add(Activation('linear'))

start = time.time()
model.compile(loss='mse', optimizer='rmsprop')
print('compilation time : {}'.format(time.time() - start))

"""
---
---
---

>> Visualização da Rede
"""

!pip3 install ann_visualizer #https://github.com/RedaOps/ann-visualizer

network = Sequential();
        #Hidden Layer#1
network.add(Dense(units=180,
                  activation='relu',
                  input_dim=X_train.shape[-1]));


        #Exit Layer
network.add(Dense(units=1,
                  activation='linear',
                  kernel_initializer='uniform'));

from ann_visualizer.visualize import ann_viz;

ann_viz(network, title="");

from ann_visualizer.visualize import ann_viz

ann_viz(network, title="LSTM") # Visualização da Rede com ANN Visualizer: compatível apenas com objetos do Keras (https://towardsdatascience.com/visualizing-artificial-neural-networks-anns-with-just-one-line-of-code-b4233607209e)



model.fit(X_train, y_train, batch_size=int(0.5*len(df)), epochs=30, validation_split=0.1)

#Visualizing training and validaton loss
plt.figure(figsize = (10, 5))
plt.plot(model.history.history['loss'], label = 'Loss')
plt.plot(model.history.history['val_loss'], Label = 'Val_Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.grid()
plt.legend()

"""> Prediction (Test)"""

#creating the list of difference between prediction and test data
loaded_model = model
diff=[]
ratio=[]  
p = loaded_model.predict(X_test)
for u in range(len(y_test)):
    pr = p[u][0]
    ratio.append((y_test[u]/pr)-1)
    diff.append(abs(y_test[u]- pr))

#Plotting the prediction and the reality (for the test data)
plt.figure(figsize = (10, 5))
plt.plot(p,color='red', label='Prediction')
plt.plot(y_test,color='blue', label='Test Data')
plt.legend(loc='upper left')
plt.grid()
plt.legend()

# Anomaly estimated population
outliers_fraction = 0.25

#Pick the most distant prediction/reality data points as anomalies
diff = pd.Series(diff)
number_of_outliers = int(outliers_fraction*len(diff))
threshold = diff.nlargest(number_of_outliers).min()

#Data with anomaly label
test = (diff >= threshold).astype(int)
complement = pd.Series(0, index=np.arange(len(data_n)-testdatasize))
df['anomaly27'] = complement.append(test, ignore_index='True')
print(df['anomaly27'].value_counts())

df.columns

df.head()

def anomaly_val(data, var):
  pos = []
  val =[]
  for i in range(0, len(data)):
    if data.anomaly27[i] == 1:
      pos.append(i)
      val.append(data[var][i])
  
  print('\n',var,':')
  print(f'\nPos: {pos}',f'\nVal: {val}')
  print('\n\n')
  
  return pos, val


rollRate = anomaly_val(df, 'rollRate')
pitchRate = anomaly_val(df, 'pitchRate')
yawRate = anomaly_val(df, 'yawRate')

"""> Exporting Model"""

import joblib
filename = "LSTM_predictions.joblib"
joblib.dump(model, filename)

loaded_LSTM = joblib.load(filename)
loaded_LSTM

y_pred = loaded_LSTM.predict(X_test)

y_pred

"""# Teste 1"""

import joblib
filename = "LSTM_predictions.joblib"

loaded_LSTM = joblib.load(filename)
loaded_LSTM

#creating the list of difference between prediction and test data
loaded_model = loaded_LSTM
diff=[]
ratio=[]  
p = loaded_model.predict(X_test)
for u in range(len(y_test)):
    pr = p[u][0]
    ratio.append((y_test[u]/pr)-1)
    diff.append(abs(y_test[u]- pr))

# Anomaly estimated population
outliers_fraction = 0.25

#Pick the most distant prediction/reality data points as anomalies
diff = pd.Series(diff)
number_of_outliers = int(outliers_fraction*len(diff))
threshold = diff.nlargest(number_of_outliers).min()

#Data with anomaly label
test = (diff >= threshold).astype(int)
complement = pd.Series(0, index=np.arange(len(data_n)-testdatasize))
df['anomaly27'] = complement.append(test, ignore_index='True')
print(df['anomaly27'].value_counts())

df.columns

def anomaly_val(data, var):
  pos = []
  val =[]
  for i in range(0, len(data)):
    if data.anomaly27[i] == 1:
      pos.append(i)
      val.append(data[var][i])
  
  print('\n',var,':')
  print(f'\nPos: {pos}',f'\nVal: {val}')
  print('\n\n')
  
  return pos, val


roll = anomaly_val(df, 'roll')
pitch = anomaly_val(df, 'pitch')
yawRate = anomaly_val(df, 'yawRate')

import os 
os.makedirs('folder/subfolder', exist_ok=True)

df2 = df.to_csv('folder/subfolder/out.csv',index=False)

"""# Teste 2

Referências:

https://stackoverflow.com/questions/67100929/lstm-valueerror-input-0-of-layer-sequential-is-incompatible-with-the-layer-ex

https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM

## Abordagem 1
"""

import joblib
filename = "LSTM_predictions.joblib"

loaded_LSTM = joblib.load(filename)
loaded_LSTM

import numpy as np
x_inst =  np.array([[
                 [1,24,50],
                ]])
x_inst.shape

y_inst = loaded_LSTM.predict(x_inst)
y_inst

"""## Abordagem 2"""

X_test.shape

import numpy as np
x_inst = np.zeros([8800, 180, 3])
x_inst[0][0] = [1,2,3]
x_inst

y_inst = np.zeros([8800,])
y_inst[0],y_inst[1],y_inst[2] = 1,2,3
y_inst

import joblib
filename = "LSTM_predictions.joblib"

loaded_LSTM = joblib.load(filename)
loaded_LSTM

#creating the list of difference between prediction and test data
diff=[]
ratio=[]  
p = loaded_LSTM.predict(x_inst)
for u in range(len(y_inst)):
    pr = p[u][0]
    ratio.append((y_inst[u]/pr)-1)
    diff.append(abs(y_inst[u]- pr))

val_1 = list(np.zeros(len(df)))
val_1[0]= 1 

val_2 = list(np.zeros(len(df)))
val_2[0] = 2 

val_3 = list(np.zeros(len(df)))
val_3[0] = 3 

df2 = pd.DataFrame({'a':val_1, 'b': val_2, 'c':val_3})
df2

# Anomaly estimated population
outliers_fraction = 0.25

#Pick the most distant prediction/reality data points as anomalies
diff = pd.Series(diff)
number_of_outliers = int(outliers_fraction*len(diff))
threshold = diff.nlargest(number_of_outliers).min()

#Data with anomaly label
test = (diff >= threshold).astype(int)
complement = pd.Series(0, index=np.arange(len(data_n)-testdatasize))
df2['anomaly27'] = complement.append(test, ignore_index='True')
print(df2['anomaly27'].value_counts())

"""# 4- Interpretabilidade

## .
"""

import warnings
warnings.filterwarnings('ignore')

df2 = pd.read_csv('out.csv')
df2.head()

rows_test = 8980
new_dimensionality = len(df2) - rows_test


df_classifier = df2.tail(new_dimensionality)
df_classifier.head()

df_classifier = df_classifier[['rollRate','pitchRate','yawRate','anomaly27']]
df_classifier

X = df_classifier.drop(['anomaly27'],axis=1)
y = df_classifier.anomaly27

print(X.head())
print('\n\n')
print(y.head())

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(criterion='entropy',n_estimators=500, max_depth=250)
# clf.fit(X_train,y_train)
clf.fit(X,y)

y_pred = clf.predict(X_test)
y_pred

from sklearn.metrics import accuracy_score
accuracy_score(y_test, y_pred)*100

"""## ."""

!pip install lime

!pip install eli5

!pip install shap

"""## Anômalo"""

print(X_test.iloc[-1])
print('Anomaly: ',y_test.iloc[-20])

import lime  #Doc: https://github.com/marcotcr/lime
import lime.lime_tabular
import numpy as np


explainer = lime.lime_tabular.LimeTabularExplainer(np.array(X_train),
                                                   feature_names=X_train.columns,
                                                   class_names=["Normal", "Anomaly"],
                                                   mode='classification')

exp = explainer.explain_instance(X_test.iloc[-20],
                                   clf.predict_proba,
                                   num_features=8,
                                   top_labels=1)

exp.show_in_notebook(show_table=True, show_all=False)

from eli5 import show_prediction

print("Actual Target Value : ", y_test.iloc[-1])
show_prediction(clf,
                X_test.iloc[-20],
                feature_names=list(X_train.columns),
                show_feature_values=True,
                )

"""## Normal"""

print(X_test.iloc[25])
print('Anomaly: ',y_test.iloc[25])

explainer = lime.lime_tabular.LimeTabularExplainer(np.array(X_train),
                                                   feature_names=X_train.columns,
                                                   class_names=["Normal", "Anomaly"],
                                                   mode='classification')

exp = explainer.explain_instance(X_test.iloc[25],
                                   clf.predict_proba,
                                   num_features=8,
                                   top_labels=1)

exp.show_in_notebook(show_table=True, show_all=False)

print("Actual Target Value : ", y_test.iloc[25])
show_prediction(clf,
                X_test.iloc[25],
                feature_names=list(X_train.columns),
                show_feature_values=True,
                )

"""## Randômico"""

import random
rand = random.randint(1, len(X_test))

print(rand)
print(X_test.iloc[rand])
print('Anomaly: ',y_test.iloc[rand])

print("Actual Target Value : ", y_test.iloc[rand])
show_prediction(clf,
                X_test.iloc[rand],
                feature_names=list(X_train.columns),
                show_feature_values=True,
                )

"""## Resumidamente"""

from eli5.formatters import format_as_html
from IPython.display import HTML
from eli5.sklearn import explain_weights_sklearn

explanation = explain_weights_sklearn(clf, feature_names=list(X_train.columns))
html_rep = format_as_html(explanation)
HTML(data=html_rep)